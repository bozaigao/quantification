#åˆ†æäºŒæ¿ä¸­éƒ½æœ‰å“ªäº›å…±åŒè§„å¾‹ï¼šå‰ä¸€æ—¥ç«ä»·æ¶¨å¹…ã€å½“æ—¥ç«ä»·æ¶¨å¹…ã€æŒ¯å¹…ã€ä¸ªè‚¡çƒ­åº¦ã€ä¸ªè‚¡å½“æ—¥ç«ä»·æ”¾é‡ç¨‹åº¦ã€é¦–æ¬¡å°æ¿æ—¶é—´ã€å¸‚å€¼å¤§å°
import os
import re
import json
from datetime import datetime, timedelta
from pandas_market_calendars import get_calendar
from utils.opening_increase import getOpeningIncrease
from  utils.judgeBurst import judgeBurst
import pychrome
import math
from colorama import Fore, Back, Style
from bs4 import BeautifulSoup

global_wait_seconds = 3
# æŒ‡å®šå›æµ‹å¹´ä»½
year = 2024
batch_size = 17
#äº¤æ˜“æ—¥æœŸ
dates = []
# è·å–ä¸­å›½äº¤æ˜“æ—¥å†
calendar = get_calendar('XSHG')  # 'XSHG' è¡¨ç¤ºä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€çš„äº¤æ˜“æ—¥å†
# åˆ›å»ºä¸€ä¸ªBrowserå®ä¾‹
browser = pychrome.Browser(url="http://127.0.0.1:9222")
# æ–°å»ºä¸€ä¸ªæ ‡ç­¾é¡µ
browserTab = browser.new_tab()
# æ‰“å¼€é“¾æ¥
browserTab.start()
browserTab.Network.enable()
strongest_pool = []
try:
    with open(f'{os.getcwd().replace("/backtest", "")}/backtest/{year}_stocks_data.json', 'r',) as file:
        stocks_data = json.load(file)
except FileNotFoundError:
    stocks_data = []
for item in stocks_data:
    dates.append(item['date'])
try:
    with open(f'{os.getcwd().replace("/backtest", "")}/backtest/{year}_second_limit.json', 'r',) as file:
        data_list = json.load(file)
except FileNotFoundError:
    data_list = []
def convert_to_number(s):
    # æ£€æŸ¥æ˜¯å¦å«æœ‰'ä¸‡'ï¼Œå¦‚æœæœ‰ï¼Œåˆ™ä¹˜ä»¥10000
    multiplier = 10000 if 'ä¸‡' in s else 1
    # ç§»é™¤æ•°å­—ä¸­çš„'ä¸‡'å’Œé€—å·
    s = re.sub('[ä¸‡,]', '', s)
    # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å¹¶æ ¹æ®æ˜¯å¦æœ‰'ä¸‡'è°ƒæ•´æ•°å€¼
    return float(s) * multiplier

# ä»æŒ‡å®šæ—¥æœŸå¼€å§‹å‘å‰æœç´¢ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥
def get_previous_trading_day(date_object):
    if str(date_object) == '2023-01-03':
        return '2022-12-30'
    while True:
        date_object -= timedelta(days=1)  # é€’å‡ä¸€å¤©
        if str(date_object) in dates:  # å¦‚æœæ˜¯äº¤æ˜“æ—¥ï¼Œåˆ™è¿”å›è¯¥æ—¥æœŸ
            return date_object

def get_jingjia_info(pre_date,current_date,stocks):
    global batch_size
    data_list = []
    for i in range(len(stocks)):
        # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å¼€å§‹å’Œç»“æŸç´¢å¼•
        start_index = i * batch_size
        end_index = start_index + batch_size
        # è·å–å½“å‰æ‰¹æ¬¡çš„è‚¡ç¥¨
        current_batch = stocks[start_index:end_index]
        # åˆå§‹åŒ–æœç´¢æ–‡æœ¬
        search_text = ''
        # æ‹¼æ¥å½“å‰æ‰¹æ¬¡çš„è‚¡ç¥¨ä»£ç 
        for item in current_batch:
            search_text += f',{item["code"]}'
        if len(search_text) <= 7:
            search_text += search_text
        # æ·»åŠ  'ç«ä»·' æ–‡å­—åˆ°æœç´¢æ–‡æœ¬ä¸­
        search_text += 'ç«ä»·'
        # æ‰“å°å½“å‰çš„æœç´¢æ–‡æœ¬ï¼Œå¯ä»¥é€‰æ‹©æ³¨é‡Šæ‰è¿™ä¸€è¡Œ
        if search_text == 'ç«ä»·':
            break
        # å¯¼èˆªåˆ°æœç´¢é¡µé¢
        browserTab.Page.navigate(url=f"https://www.iwencai.com/stockpick/search?rsh=3&typed=1&preParams=&ts=1&f=1&qs=result_rewrite&selfsectsn=&querytype=stock&searchfilter=&tid=stockpick&w={str(pre_date)}{search_text}")
        # ç­‰å¾…é¡µé¢åŠ è½½
        browserTab.wait(global_wait_seconds)
        result = browserTab.Runtime.evaluate(expression="document.documentElement.outerHTML")
        soup = BeautifulSoup(result['result']['value'], 'html.parser')
        # æ‰¾åˆ°åŒ…å«è‚¡ç¥¨ä»£ç å’Œç®€ç§°çš„è¡¨æ ¼
        table = soup.find('table', class_='static_table tbody_table static_tbody_table')
        # å­˜å‚¨ç»“æœçš„åˆ—è¡¨
        stocks_data = []
        # éå†è¡¨æ ¼ä¸­çš„æ¯ä¸€è¡Œ
        for row in table.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) >= 4:  # ç¡®ä¿æ¯è¡Œä¸­æœ‰è¶³å¤Ÿçš„å•å…ƒæ ¼æ•°æ®
                # æå–è‚¡ç¥¨ä»£ç å’Œç®€ç§°
                stock_code = cells[2].text.strip()
                stock_name_link = cells[3].find('a')
                if stock_name_link:
                    stock_name = stock_name_link.text.strip()
                    stocks_data.append({
                        'code': stock_code,
                        'name': stock_name
                    })
        table = soup.find('table', class_='scroll_table tbody_table scroll_tbody_table')
        # éå†è¡¨æ ¼ä¸­çš„æ¯ä¸€è¡Œ
        for index,row in enumerate(table.find_all('tr')):
            # æŒ‰é¡ºåºè·å–åˆ—æ•°æ®
            columns = row.find_all('td')
            if len(columns) >= 12:  # ç¡®ä¿æ¯è¡Œæœ‰è¶³å¤Ÿçš„æ•°æ®åˆ—
                bidding_increase = columns[2].text.strip()  # ç«ä»·æ¶¨å¹…
                bidding_volume = columns[7].text.strip()  # ç«ä»·é‡
                bidding_amount = columns[8].text.strip()  # ç«ä»·é‡‘é¢
                # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                if pre_date == current_date:
                    data_list.append({
                        'code': stocks_data[index]["code"],
                        'name': stocks_data[index]["name"],
                        'bidding_increase': f'{bidding_increase}%',
                        'bidding_volume': bidding_volume,
                        'bidding_amount': bidding_amount
                    })
                else:
                     data_list.append({
                        'code': stocks_data[index]["code"],
                        'name': stocks_data[index]["name"],
                        'pre_bidding_increase': f'{bidding_increase}%',
                        'pre_bidding_volume': bidding_volume,
                        'pre_bidding_amount': bidding_amount
                    })
    return data_list

if len(data_list) == 0:
    stocks_data = stocks_data[:1]
    for item in stocks_data:
        filter_stocks = [stock for stock in item['data'] if stock['limit'] == 2]
        # å­˜å‚¨ç»“æœçš„åˆ—è¡¨
        pre_date = get_previous_trading_day(datetime.strptime(item['date'], '%Y-%m-%d').date())
        data_list = get_jingjia_info(pre_date,item['date'],filter_stocks)
        #è·å–ç«ä»·ä¿¡æ¯
        hot_stocks_data = []
        print(f'ğŸ˜1{item}')
        # è·å–è‚¡ç¥¨çƒ­åº¦ä¿¡æ¯
        for i in range(len(data_list)):
            # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„å¼€å§‹å’Œç»“æŸç´¢å¼•
            start_index = i * batch_size
            end_index = start_index + batch_size
            # è·å–å½“å‰æ‰¹æ¬¡çš„è‚¡ç¥¨
            current_batch = data_list[start_index:end_index]
            # åˆå§‹åŒ–æœç´¢æ–‡æœ¬
            search_text = ''
            # æ‹¼æ¥å½“å‰æ‰¹æ¬¡çš„è‚¡ç¥¨ä»£ç 
            for item in current_batch:
                search_text += f',{item["code"]}'
            # æ·»åŠ  'ç«ä»·' æ–‡å­—åˆ°æœç´¢æ–‡æœ¬ä¸­
            search_text += 'çƒ­åº¦'
            # æ‰“å°å½“å‰çš„æœç´¢æ–‡æœ¬ï¼Œå¯ä»¥é€‰æ‹©æ³¨é‡Šæ‰è¿™ä¸€è¡Œ
            if search_text == 'çƒ­åº¦':
                break
            # å¯¼èˆªåˆ°æœç´¢é¡µé¢
            browserTab.Page.navigate(url=f"https://www.iwencai.com/stockpick/search?rsh=3&typed=1&preParams=&ts=1&f=1&qs=result_rewrite&selfsectsn=&querytype=stock&searchfilter=&tid=stockpick&w={str(pre_date)}{search_text}")
            # ç­‰å¾…é¡µé¢åŠ è½½
            browserTab.wait(global_wait_seconds)
            result = browserTab.Runtime.evaluate(expression="document.documentElement.outerHTML")
            soup = BeautifulSoup(result['result']['value'], 'html.parser')
            # åˆå§‹åŒ–åˆ—è¡¨æ¥ä¿å­˜æ¯åªè‚¡ç¥¨çš„ä»£ç å’Œç®€ç§°
            stock_info = []
            # å…ˆæ‰¾åˆ°å…·ä½“çš„<table>æ ‡ç­¾
            table = soup.find('table', class_='static_table tbody_table static_tbody_table')
            # åœ¨æ‰¾åˆ°çš„<table>ä¸­éå†<tbody>ä¸­çš„<tr>æ ‡ç­¾
            for row in table.find('tbody').find_all('tr'):
                cells = row.find_all('td')
                if len(cells) >= 4:  # ç¡®ä¿<td>æ ‡ç­¾çš„æ•°é‡è¶³å¤Ÿ
                    stock_code = cells[2].get_text(strip=True)  # è·å–è‚¡ç¥¨ä»£ç 
                    stock_name = cells[3].text.strip()  # è·å–è‚¡ç¥¨ç®€ç§°ï¼Œä½¿ç”¨.text.strip()æ›´æ¸…æ™°
                    stock_info.append({
                            'code': stock_code,
                            'name': stock_name
                        })
            # å®šä½åˆ°å…·ä½“çš„<table>æ ‡ç­¾
            table = soup.find('table', class_='scroll_table tbody_table scroll_tbody_table')
            # åœ¨æ‰¾åˆ°çš„<table>ä¸­éå†<tbody>ä¸­çš„<tr>æ ‡ç­¾
            tbody = table.find('tbody')
            for index, row in enumerate(tbody.find_all('tr')):
                cells = row.find_all('td')
                if len(cells) >= 5:  # ç¡®ä¿åˆ—æ•°è¶³å¤Ÿ
                    volume = cells[2].get_text(strip=True)  # äº¤æ˜“é‡ï¼Œå³â€œçƒ­åº¦â€
                    rank = cells[3].get_text(strip=True)    # æ’å
                    hot_stocks_data.append({"code":stock_info[index]["code"],"name":stock_info[index]["name"], "rank":rank,"volume":volume})
        for item in data_list:
            for item2 in hot_stocks_data:
                if item["code"] == item2["code"]:
                    item["rank"] = item2["rank"].replace(',', '')
                    item["volume"] = item2["volume"]
        for item in data_list:
            for item2 in filter_stocks:
                if item2["code"] == item["code"]:
                    item2["rank"] = item["rank"].replace(',', '')
                    item2["volume"] = item["volume"]
                    item2["pre_bidding_increase"] = item["pre_bidding_increase"]
                    item2["pre_bidding_volume"] = item["pre_bidding_volume"]
                    item2["pre_bidding_amount"] = item["pre_bidding_amount"]
        print(item)
        next_data_list = get_jingjia_info(pre_date, item['date'], data_list)
        for item in next_data_list:
            for item2 in filter_stocks:
                if item2["code"] == item["code"]:
                    item2["rank"] = item["rank"].replace(',', '')
                    item2["volume"] = item["volume"]
                    item2["bidding_increase"] = item["bidding_increase"]
                    item2["bidding_volume"] = item["bidding_volume"]
                    item2["bidding_amount"] = item["bidding_amount"]
        with open(f'{os.getcwd().replace("/backtest", "")}/backtest/{year}_second_limit_analys.json', 'w') as file:
                json.dump(filter_stocks, file,ensure_ascii=False,  indent=4) 
# for index1, item in enumerate(data_list):
#     for index2, item in enumerate(next_data_list):
#         if data_list[index1]["code"] == next_data_list[index2]["code"]:
#             data_list[index1]["next_bidding_increase"] = next_data_list[index2]["bidding_increase"]
#             data_list[index1]["next_bidding_volume"] = next_data_list[index2]["bidding_volume"]
#             data_list[index1]["next_bidding_amount"] = next_data_list[index2]["bidding_amount"]
# with open(f'{os.getcwd().replace("/backtest", "")}/backtest/today_increase.json', 'w') as file:
#     json.dump(data_list, file,ensure_ascii=False,  indent=4) 
# for item in data_list:
#     pre_opening_increase = float(item["bidding_increase"].strip('%'))
#     current_opening_increase = float(item["next_bidding_increase"].strip('%'))
   
#     if pre_opening_increase >= 9.5 and current_opening_increase >= 9.5 and abs(pre_opening_increase - current_opening_increase) <= 0.5:
#         bothIsLimitPrice = True
#     else:
#         bothIsLimitPrice = False
#     if (current_opening_increase > pre_opening_increase or bothIsLimitPrice) and current_opening_increase > 0:
#         strongest_pool.append({'date':str(find_date),'name':item['name'],'code':item['code'],'pre_opening_increase':pre_opening_increase,'current_opening_increase':current_opening_increase,'rank':item['rank'],'bidding_volume':item['bidding_volume'],'next_bidding_volume':item['next_bidding_volume']})
               
# strongest_pool = sorted(strongest_pool, key=lambda x: (-x['current_opening_increase'], int(x['rank'])))
# print(f'ä»{len(data_list)}ä¸ªè‚¡ç¥¨ä¸­ç­›é€‰å‡º{len(strongest_pool)}æ”¯ä¸ªè‚¡')
# for index, item in enumerate(strongest_pool):
#     print(Fore.GREEN + f'{index+1}.{item["name"]},æ˜¨æ—¥ç«ä»·{item["pre_opening_increase"]}%,å½“æ—¥ç«ä»·{Fore.RED}{item["current_opening_increase"]}% {Fore.GREEN},æŒ¯å¹…{Fore.RED}{round(abs(item["current_opening_increase"] - item["pre_opening_increase"]),2)}%{Fore.GREEN},çƒ­åº¦æ’å:{Fore.RED}{item["rank"]}{Fore.GREEN},æ”¾é‡ç³»æ•°:{Fore.RED}{round(convert_to_number(item["next_bidding_volume"])/convert_to_number(item["bidding_volume"]),2)}')
# get_jingjia_info(find_date, strongest_pool)